# 德州扑克发牌算法AI测试系统 - 技术栈与开发路线图

## 1. 技术栈选型

### 1.1 后端技术栈

#### 1.1.1 核心编程语言

**Python 3.10+** (主要开发语言)
- 优势: 丰富的AI/ML生态、开发效率高
- 应用场景:
  - AI模型训练与推理
  - 数据分析与处理
  - 玩家行为分析
  - 业务逻辑实现

**Go 1.20+** (高性能服务)
- 优势: 高并发性能、低延迟
- 应用场景:
  - 发牌引擎核心服务
  - WebSocket实时通信
  - API网关
  - 高频交易处理

#### 1.1.2 Web框架

**Python生态**:
```python
# FastAPI - 高性能异步API框架
from fastapi import FastAPI, WebSocket
from pydantic import BaseModel

app = FastAPI()

@app.post("/api/v1/games/deal")
async def deal_cards(game_id: str):
    # 异步处理发牌请求
    pass

# Django - 管理后台和完整Web应用
# 用于快速搭建后台管理系统
```

**Go生态**:
```go
// Gin - 高性能HTTP框架
package main

import "github.com/gin-gonic/gin"

func main() {
    r := gin.Default()
    r.POST("/api/v1/games/deal", dealHandler)
    r.Run(":8080")
}
```

#### 1.1.3 AI/ML框架

**PyTorch 2.0+**
```python
import torch
import torch.nn as nn

class PlayerBehaviorModel(nn.Module):
    """玩家行为预测模型"""
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])
```

**Scikit-learn**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans

# 玩家类型分类
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)

# 玩家聚类
kmeans = KMeans(n_clusters=5)
player_clusters = kmeans.fit_predict(player_features)
```

**Ray / Ray RLlib** (分布式训练)
```python
import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPO

# 分布式强化学习训练
ray.init()
config = {
    "env": "PokerEnv",
    "num_gpus": 1,
    "num_workers": 4,
}
tune.run(PPO, config=config)
```

### 1.2 数据存储技术栈

#### 1.2.1 关系型数据库

**PostgreSQL 15+**
```sql
-- 使用JSONB存储复杂数据
CREATE TABLE hands (
    hand_id UUID PRIMARY KEY,
    player_id UUID,
    actions JSONB,
    metadata JSONB
);

-- 使用GIN索引加速JSONB查询
CREATE INDEX idx_actions_gin ON hands USING GIN(actions);

-- 分区表提升查询性能
CREATE TABLE games (
    game_id UUID,
    started_at TIMESTAMP,
    ...
) PARTITION BY RANGE (started_at);

CREATE TABLE games_2025_01 PARTITION OF games
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

**连接池配置**:
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "postgresql://user:pass@localhost/poker",
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)
```

#### 1.2.2 NoSQL数据库

**MongoDB 6.0+**
```javascript
// 使用复合索引优化查询
db.game_records.createIndex(
    { game_id: 1, timestamp: -1 }
)

// 使用聚合管道进行复杂分析
db.game_records.aggregate([
    { $match: { player_id: "xxx" } },
    { $unwind: "$streets.flop.actions" },
    { $group: {
        _id: "$streets.flop.actions.action",
        count: { $sum: 1 }
    }}
])

// 使用TTL索引自动清理过期数据
db.sessions.createIndex(
    { "created_at": 1 },
    { expireAfterSeconds: 86400 }
)
```

**Python集成**:
```python
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import ASCENDING, DESCENDING

client = AsyncIOMotorClient("mongodb://localhost:27017")
db = client.poker_db

# 异步查询
async def get_game_history(game_id: str):
    return await db.game_records.find_one({"game_id": game_id})
```

#### 1.2.3 缓存系统

**Redis 7.0+**
```python
import redis
from redis.asyncio import Redis
import json

# 同步Redis客户端
r = redis.Redis(host='localhost', port=6379, db=0)

# 玩家实时统计缓存
def cache_player_stats(player_id: str, stats: dict):
    key = f"player:stats:{player_id}"
    r.setex(key, 3600, json.dumps(stats))

# 使用Redis Streams处理实时事件
def publish_game_event(event: dict):
    r.xadd("game_events", event)

# 使用Redis Sorted Set实现排行榜
def update_leaderboard(player_id: str, score: float):
    r.zadd("leaderboard", {player_id: score})

# 异步Redis (高性能)
async def async_get_player_stats(player_id: str):
    async_redis = Redis(host='localhost', port=6379)
    key = f"player:stats:{player_id}"
    data = await async_redis.get(key)
    return json.loads(data) if data else None
```

**Redis集群配置**:
```python
from redis.cluster import RedisCluster

startup_nodes = [
    {"host": "redis-node-1", "port": "6379"},
    {"host": "redis-node-2", "port": "6379"},
    {"host": "redis-node-3", "port": "6379"}
]

rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)
```

#### 1.2.4 时序数据库

**InfluxDB 2.x**
```python
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS

client = InfluxDBClient(url="http://localhost:8086", token="my-token", org="poker-ai")
write_api = client.write_api(write_options=SYNCHRONOUS)

# 写入性能指标
def record_metric(metric_name: str, value: float, tags: dict):
    point = Point(metric_name) \
        .tag("service", tags.get("service")) \
        .field("value", value) \
        .time(datetime.utcnow())

    write_api.write(bucket="metrics", record=point)

# 查询时序数据
query = '''
from(bucket: "metrics")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "api_latency")
  |> mean()
'''
result = client.query_api().query(query)
```

#### 1.2.5 消息队列

**Apache Kafka**
```python
from kafka import KafkaProducer, KafkaConsumer
import json

# 生产者
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

def publish_game_event(event: dict):
    producer.send('game-events', value=event)

# 消费者
consumer = KafkaConsumer(
    'game-events',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    process_game_event(message.value)
```

**RabbitMQ** (轻量级任务队列)
```python
import pika

# 发布任务
connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()
channel.queue_declare(queue='training_tasks')

channel.basic_publish(
    exchange='',
    routing_key='training_tasks',
    body=json.dumps(task_data)
)

# 消费任务
def callback(ch, method, properties, body):
    task = json.loads(body)
    process_training_task(task)

channel.basic_consume(
    queue='training_tasks',
    on_message_callback=callback,
    auto_ack=True
)
channel.start_consuming()
```

#### 1.2.6 对象存储

**MinIO**
```python
from minio import Minio

# 初始化客户端
minio_client = Minio(
    "localhost:9000",
    access_key="minioadmin",
    secret_key="minioadmin",
    secure=False
)

# 上传模型文件
def upload_model(model_path: str, model_name: str):
    minio_client.fput_object(
        "models",
        model_name,
        model_path
    )

# 下载模型文件
def download_model(model_name: str, local_path: str):
    minio_client.fget_object(
        "models",
        model_name,
        local_path
    )
```

### 1.3 前端技术栈

#### 1.3.1 框架选择

**React 18+**
```jsx
import React, { useState, useEffect } from 'react';
import { useWebSocket } from 'react-use-websocket';

function GameTable() {
    const [gameState, setGameState] = useState(null);
    const { lastMessage } = useWebSocket('ws://api.example.com/ws/game');

    useEffect(() => {
        if (lastMessage !== null) {
            const data = JSON.parse(lastMessage.data);
            setGameState(data);
        }
    }, [lastMessage]);

    return (
        <div className="game-table">
            {/* 游戏界面 */}
        </div>
    );
}
```

#### 1.3.2 可视化库

**ECharts / D3.js**
```javascript
import * as echarts from 'echarts';

// 玩家胜率趋势图
const option = {
    title: { text: '玩家胜率趋势' },
    xAxis: { type: 'category', data: dates },
    yAxis: { type: 'value' },
    series: [{
        data: winRates,
        type: 'line',
        smooth: true
    }]
};

const chart = echarts.init(document.getElementById('chart'));
chart.setOption(option);
```

**Recharts** (React专用)
```jsx
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip } from 'recharts';

function WinRateChart({ data }) {
    return (
        <LineChart width={600} height={300} data={data}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis dataKey="date" />
            <YAxis />
            <Tooltip />
            <Line type="monotone" dataKey="winRate" stroke="#8884d8" />
        </LineChart>
    );
}
```

### 1.4 DevOps技术栈

#### 1.4.1 容器化

**Docker**
```dockerfile
# Python服务Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Docker Compose** (本地开发)
```yaml
version: '3.8'

services:
  api:
    build: ./api
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/poker
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: poker
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  mongodb:
    image: mongo:6
    volumes:
      - mongo_data:/data/db

volumes:
  postgres_data:
  mongo_data:
```

#### 1.4.2 容器编排

**Kubernetes**
```yaml
# 完整部署配置
---
apiVersion: v1
kind: Namespace
metadata:
  name: poker-ai

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dealing-engine
  namespace: poker-ai
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dealing-engine
  template:
    metadata:
      labels:
        app: dealing-engine
        version: v1.0
    spec:
      containers:
      - name: dealing-engine
        image: poker-ai/dealing-engine:v1.0
        ports:
        - containerPort: 8080
          name: http
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: redis_url
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: dealing-engine-svc
  namespace: poker-ai
spec:
  selector:
    app: dealing-engine
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dealing-engine-hpa
  namespace: poker-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dealing-engine
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### 1.4.3 CI/CD

**GitLab CI**
```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - deploy

variables:
  DOCKER_REGISTRY: registry.example.com
  IMAGE_NAME: poker-ai/dealing-engine

test:
  stage: test
  image: python:3.10
  script:
    - pip install -r requirements.txt
    - pytest tests/ --cov=app --cov-report=xml
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $DOCKER_REGISTRY
    - docker build -t $IMAGE_NAME:$CI_COMMIT_SHA .
    - docker tag $IMAGE_NAME:$CI_COMMIT_SHA $IMAGE_NAME:latest
    - docker push $IMAGE_NAME:$CI_COMMIT_SHA
    - docker push $IMAGE_NAME:latest
  only:
    - main

deploy-staging:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context staging
    - kubectl set image deployment/dealing-engine dealing-engine=$IMAGE_NAME:$CI_COMMIT_SHA -n poker-ai
    - kubectl rollout status deployment/dealing-engine -n poker-ai
  environment:
    name: staging
    url: https://staging.poker-ai.example.com
  only:
    - main

deploy-production:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context production
    - kubectl set image deployment/dealing-engine dealing-engine=$IMAGE_NAME:$CI_COMMIT_SHA -n poker-ai
    - kubectl rollout status deployment/dealing-engine -n poker-ai
  environment:
    name: production
    url: https://poker-ai.example.com
  when: manual
  only:
    - main
```

#### 1.4.4 监控与日志

**Prometheus + Grafana**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'dealing-engine'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - poker-ai
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: dealing-engine
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod_name

  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
```

**应用内嵌Prometheus指标**:
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# 定义指标
deal_requests_total = Counter('deal_requests_total', 'Total dealing requests')
deal_latency = Histogram('deal_latency_seconds', 'Dealing latency')
active_games = Gauge('active_games', 'Number of active games')

@deal_latency.time()
async def deal_cards(game_id: str):
    deal_requests_total.inc()
    # 发牌逻辑
    pass

# 启动metrics服务器
start_http_server(8001)
```

**ELK Stack** (日志管理)
```python
import logging
from pythonjsonlogger import jsonlogger

# 配置JSON格式日志
logHandler = logging.StreamHandler()
formatter = jsonlogger.JsonFormatter(
    '%(timestamp)s %(level)s %(name)s %(message)s'
)
logHandler.setFormatter(formatter)

logger = logging.getLogger()
logger.addHandler(logHandler)
logger.setLevel(logging.INFO)

# 结构化日志
logger.info(
    "Game started",
    extra={
        "game_id": "xxx",
        "num_players": 6,
        "algorithm_version": "v1.2"
    }
)
```

---

## 2. 详细开发路线图

### 2.1 第一阶段: 基础框架搭建 (Week 1-8)

#### Week 1-2: 项目初始化
```bash
# 任务清单
✓ 创建代码仓库
✓ 设置开发环境
✓ 配置CI/CD流水线
✓ 搭建基础项目结构

# 项目结构
poker-ai-system/
├── services/
│   ├── dealing-engine/      # 发牌引擎服务
│   ├── player-analysis/     # 玩家分析服务
│   ├── ml-training/         # AI训练服务
│   └── api-gateway/         # API网关
├── libs/
│   ├── common/              # 公共库
│   └── models/              # 数据模型
├── infra/
│   ├── k8s/                 # Kubernetes配置
│   ├── terraform/           # 基础设施即代码
│   └── docker/              # Docker配置
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
└── docs/                    # 文档
```

#### Week 3-4: 数据模型与数据库设计
```python
# 任务清单
✓ 设计核心数据模型
✓ 创建PostgreSQL数据库schema
✓ 配置MongoDB集合
✓ 设置Redis缓存策略
✓ 编写数据访问层代码

# 代码示例: 数据模型定义
from sqlalchemy import Column, String, Integer, DECIMAL, JSON
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Player(Base):
    __tablename__ = 'players'

    player_id = Column(UUID(as_uuid=True), primary_key=True)
    nickname = Column(String(50), nullable=False)
    level = Column(Integer, default=1)
    total_games = Column(Integer, default=0)
    win_rate = Column(DECIMAL(5, 2))
    skill_score = Column(Integer, default=50)
    player_type = Column(String(20))

class Game(Base):
    __tablename__ = 'games'

    game_id = Column(UUID(as_uuid=True), primary_key=True)
    table_id = Column(UUID(as_uuid=True), nullable=False)
    small_blind = Column(DECIMAL(10, 2))
    big_blind = Column(DECIMAL(10, 2))
    algorithm_version = Column(String(20))

# ORM操作封装
from sqlalchemy.orm import Session

class PlayerRepository:
    def __init__(self, session: Session):
        self.session = session

    def get_by_id(self, player_id: str) -> Player:
        return self.session.query(Player).filter_by(player_id=player_id).first()

    def create(self, player: Player) -> Player:
        self.session.add(player)
        self.session.commit()
        return player

    def update_stats(self, player_id: str, stats: dict):
        player = self.get_by_id(player_id)
        for key, value in stats.items():
            setattr(player, key, value)
        self.session.commit()
```

#### Week 5-6: 基础发牌引擎
```python
# 任务清单
✓ 实现标准德州扑克规则
✓ 实现RNG随机数生成器
✓ 实现洗牌和发牌逻辑
✓ 编写单元测试

# 核心代码实现
from enum import Enum
import random
from typing import List
from dataclasses import dataclass

class Suit(Enum):
    HEARTS = "♥"
    DIAMONDS = "♦"
    CLUBS = "♣"
    SPADES = "♠"

class Rank(Enum):
    TWO = 2
    THREE = 3
    FOUR = 4
    FIVE = 5
    SIX = 6
    SEVEN = 7
    EIGHT = 8
    NINE = 9
    TEN = 10
    JACK = 11
    QUEEN = 12
    KING = 13
    ACE = 14

@dataclass
class Card:
    suit: Suit
    rank: Rank

    def __str__(self):
        rank_map = {
            Rank.ACE: 'A', Rank.KING: 'K', Rank.QUEEN: 'Q',
            Rank.JACK: 'J', Rank.TEN: 'T'
        }
        rank_str = rank_map.get(self.rank, str(self.rank.value))
        return f"{rank_str}{self.suit.value}"

class Deck:
    """牌堆"""

    def __init__(self):
        self.cards: List[Card] = []
        self.reset()

    def reset(self):
        """重置牌堆"""
        self.cards = [
            Card(suit, rank)
            for suit in Suit
            for rank in Rank
        ]

    def shuffle(self):
        """洗牌 (Fisher-Yates算法)"""
        for i in range(len(self.cards) - 1, 0, -1):
            j = random.randint(0, i)
            self.cards[i], self.cards[j] = self.cards[j], self.cards[i]

    def deal(self, n: int = 1) -> List[Card]:
        """发牌"""
        if len(self.cards) < n:
            raise ValueError("Not enough cards in deck")
        dealt_cards = self.cards[:n]
        self.cards = self.cards[n:]
        return dealt_cards

class DealingEngine:
    """发牌引擎"""

    def __init__(self):
        self.deck = Deck()

    def start_new_hand(self, num_players: int) -> dict:
        """开始新的一手牌"""
        if not 2 <= num_players <= 10:
            raise ValueError("Number of players must be between 2 and 10")

        # 重置并洗牌
        self.deck.reset()
        self.deck.shuffle()

        # 给每个玩家发两张底牌
        hole_cards = [self.deck.deal(2) for _ in range(num_players)]

        return {
            "hole_cards": hole_cards,
            "deck_remaining": len(self.deck.cards)
        }

    def deal_flop(self) -> List[Card]:
        """发翻牌 (3张)"""
        self.deck.deal(1)  # Burn card
        return self.deck.deal(3)

    def deal_turn(self) -> Card:
        """发转牌"""
        self.deck.deal(1)  # Burn card
        return self.deck.deal(1)[0]

    def deal_river(self) -> Card:
        """发河牌"""
        self.deck.deal(1)  # Burn card
        return self.deck.deal(1)[0]

# 单元测试
import unittest

class TestDealingEngine(unittest.TestCase):
    def setUp(self):
        self.engine = DealingEngine()

    def test_deck_has_52_cards(self):
        self.assertEqual(len(self.engine.deck.cards), 52)

    def test_deal_hole_cards(self):
        result = self.engine.start_new_hand(6)
        self.assertEqual(len(result["hole_cards"]), 6)
        self.assertEqual(len(result["hole_cards"][0]), 2)

    def test_deal_flop(self):
        self.engine.start_new_hand(6)
        flop = self.engine.deal_flop()
        self.assertEqual(len(flop), 3)

    def test_card_uniqueness(self):
        """确保发出的牌不重复"""
        result = self.engine.start_new_hand(10)
        all_cards = [card for hand in result["hole_cards"] for card in hand]
        self.assertEqual(len(all_cards), len(set(all_cards)))
```

#### Week 7-8: API开发与集成
```python
# 任务清单
✓ 开发RESTful API
✓ 实现WebSocket实时通信
✓ API文档自动生成
✓ 集成测试

# FastAPI实现
from fastapi import FastAPI, WebSocket, HTTPException
from pydantic import BaseModel
from typing import List
import asyncio

app = FastAPI(title="Poker AI System API", version="1.0.0")

class GameCreateRequest(BaseModel):
    num_players: int
    small_blind: float
    big_blind: float

class GameResponse(BaseModel):
    game_id: str
    status: str
    players: List[dict]

@app.post("/api/v1/games", response_model=GameResponse)
async def create_game(request: GameCreateRequest):
    """创建新游戏"""
    if not 2 <= request.num_players <= 10:
        raise HTTPException(status_code=400, detail="Invalid number of players")

    # 创建游戏逻辑
    game = await game_service.create_game(request)
    return game

@app.post("/api/v1/games/{game_id}/deal")
async def deal_cards(game_id: str):
    """发牌"""
    result = await dealing_service.deal(game_id)
    return result

# WebSocket实现
class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, List[WebSocket]] = {}

    async def connect(self, game_id: str, websocket: WebSocket):
        await websocket.accept()
        if game_id not in self.active_connections:
            self.active_connections[game_id] = []
        self.active_connections[game_id].append(websocket)

    async def broadcast(self, game_id: str, message: dict):
        if game_id in self.active_connections:
            for connection in self.active_connections[game_id]:
                await connection.send_json(message)

manager = ConnectionManager()

@app.websocket("/ws/game/{game_id}")
async def game_websocket(websocket: WebSocket, game_id: str):
    await manager.connect(game_id, websocket)
    try:
        while True:
            data = await websocket.receive_json()
            # 处理玩家动作
            result = await process_player_action(game_id, data)
            # 广播更新
            await manager.broadcast(game_id, result)
    except Exception as e:
        print(f"WebSocket error: {e}")
```

### 2.2 第二阶段: 核心功能开发 (Week 9-20)

#### Week 9-11: 玩家行为追踪系统
```python
# 任务清单
✓ 实现行为事件采集
✓ 实时统计计算
✓ 数据持久化
✓ 行为模式检测

# 实现代码
from dataclasses import dataclass
from datetime import datetime
from collections import deque
import numpy as np

@dataclass
class ActionEvent:
    """行动事件"""
    player_id: str
    game_id: str
    street: str  # preflop/flop/turn/river
    action_type: str  # fold/call/raise/check
    amount: float
    pot_size: float
    position: int
    timestamp: datetime

class BehaviorTracker:
    """行为追踪器"""

    def __init__(self):
        self.action_history: dict[str, deque] = {}
        self.stats_calculator = StatsCalculator()

    async def track_action(self, event: ActionEvent):
        """追踪玩家行为"""
        player_id = event.player_id

        # 初始化历史记录
        if player_id not in self.action_history:
            self.action_history[player_id] = deque(maxlen=1000)

        # 添加事件
        self.action_history[player_id].append(event)

        # 更新统计数据
        stats = self.stats_calculator.calculate(player_id, self.action_history[player_id])

        # 缓存到Redis
        await self._cache_stats(player_id, stats)

        # 异步存储到数据库
        await self._persist_event(event)

        return stats

    async def _cache_stats(self, player_id: str, stats: dict):
        """缓存统计数据"""
        key = f"player:stats:{player_id}"
        await redis.setex(key, 3600, json.dumps(stats))

    async def _persist_event(self, event: ActionEvent):
        """持久化事件"""
        await action_repository.create(event)

class StatsCalculator:
    """统计计算器"""

    def calculate(self, player_id: str, actions: deque) -> dict:
        """计算玩家统计数据"""
        if not actions:
            return self._default_stats()

        # VPIP: 自愿入池率
        vpip = self._calculate_vpip(actions)

        # PFR: 翻牌前加注率
        pfr = self._calculate_pfr(actions)

        # 3-Bet率
        three_bet_rate = self._calculate_3bet_rate(actions)

        # 激进因子
        aggression_factor = self._calculate_aggression_factor(actions)

        # 平均决策时间
        avg_decision_time = self._calculate_avg_decision_time(actions)

        return {
            "vpip": vpip,
            "pfr": pfr,
            "three_bet_rate": three_bet_rate,
            "aggression_factor": aggression_factor,
            "avg_decision_time_ms": avg_decision_time,
            "total_hands": len(actions)
        }

    def _calculate_vpip(self, actions: deque) -> float:
        """计算VPIP"""
        preflop_actions = [a for a in actions if a.street == "preflop"]
        if not preflop_actions:
            return 0.0

        voluntary_puts = sum(
            1 for a in preflop_actions
            if a.action_type in ["call", "raise"]
        )

        return (voluntary_puts / len(preflop_actions)) * 100

    def _calculate_pfr(self, actions: deque) -> float:
        """计算PFR"""
        preflop_actions = [a for a in actions if a.street == "preflop"]
        if not preflop_actions:
            return 0.0

        raises = sum(1 for a in preflop_actions if a.action_type == "raise")
        return (raises / len(preflop_actions)) * 100

    def _calculate_aggression_factor(self, actions: deque) -> float:
        """计算激进因子: (raise + bet) / call"""
        aggressive_actions = sum(
            1 for a in actions
            if a.action_type in ["raise", "bet"]
        )
        passive_actions = sum(1 for a in actions if a.action_type == "call")

        if passive_actions == 0:
            return float('inf') if aggressive_actions > 0 else 0.0

        return aggressive_actions / passive_actions
```

#### Week 12-14: 玩家画像系统
```python
# 任务清单
✓ 玩家类型分类模型
✓ 技术水平评估
✓ 打法风格识别
✓ 实时画像更新

# 实现代码
from enum import Enum
from sklearn.ensemble import RandomForestClassifier
import joblib

class PlayerType(Enum):
    TAG = "紧凶型"  # Tight Aggressive
    LAG = "松凶型"  # Loose Aggressive
    TIGHT_PASSIVE = "紧弱型"
    LOOSE_PASSIVE = "松弱型"
    UNKNOWN = "未知"

class PlayerProfiler:
    """玩家画像"""

    def __init__(self):
        self.classifier = self._load_or_train_classifier()

    def classify_player_type(self, stats: dict) -> PlayerType:
        """分类玩家类型"""
        vpip = stats.get("vpip", 0)
        pfr = stats.get("pfr", 0)

        # 规则引擎分类
        if vpip < 20:
            if pfr / vpip > 0.7 if vpip > 0 else False:
                return PlayerType.TAG
            else:
                return PlayerType.TIGHT_PASSIVE
        else:
            if pfr > 20:
                return PlayerType.LAG
            else:
                return PlayerType.LOOSE_PASSIVE

    def evaluate_skill_level(self, player_id: str) -> int:
        """评估技术水平 (0-100分)"""
        # 获取玩家数据
        stats = self._get_player_stats(player_id)
        actions = self._get_player_actions(player_id)

        score = 0

        # 1. 基础策略理解 (30分)
        score += self._evaluate_basic_strategy(stats)

        # 2. 位置意识 (20分)
        score += self._evaluate_position_awareness(actions)

        # 3. 读牌能力 (25分)
        score += self._evaluate_hand_reading(actions)

        # 4. 资金管理 (25分)
        score += self._evaluate_bankroll_management(actions)

        return min(score, 100)

    def _evaluate_basic_strategy(self, stats: dict) -> int:
        """评估基础策略"""
        score = 0
        vpip = stats.get("vpip", 0)
        pfr = stats.get("pfr", 0)

        # 理想的VPIP范围: 15-25
        if 15 <= vpip <= 25:
            score += 15
        elif 10 <= vpip < 15 or 25 < vpip <= 30:
            score += 10
        elif vpip < 10 or vpip > 30:
            score += 5

        # 理想的PFR范围: 10-20
        if 10 <= pfr <= 20:
            score += 15
        elif 5 <= pfr < 10 or 20 < pfr <= 25:
            score += 10
        elif pfr < 5 or pfr > 25:
            score += 5

        return score

    def _evaluate_position_awareness(self, actions: List[ActionEvent]) -> int:
        """评估位置意识"""
        # 分析不同位置的打法差异
        early_position_actions = [a for a in actions if a.position <= 2]
        late_position_actions = [a for a in actions if a.position >= 6]

        if not early_position_actions or not late_position_actions:
            return 0

        # 计算不同位置的VPIP
        early_vpip = self._calculate_position_vpip(early_position_actions)
        late_vpip = self._calculate_position_vpip(late_position_actions)

        # 晚位VPIP应该高于早位
        if late_vpip > early_vpip * 1.3:
            return 20
        elif late_vpip > early_vpip:
            return 15
        else:
            return 5

    def generate_profile_report(self, player_id: str) -> dict:
        """生成完整画像报告"""
        stats = self._get_player_stats(player_id)

        return {
            "player_id": player_id,
            "player_type": self.classify_player_type(stats).value,
            "skill_level": self.evaluate_skill_level(player_id),
            "statistics": stats,
            "strengths": self._identify_strengths(stats),
            "weaknesses": self._identify_weaknesses(stats),
            "recommendations": self._generate_recommendations(stats)
        }
```

#### Week 15-17: 智能发牌算法v1.0
```python
# 任务清单
✓ 设计智能发牌策略
✓ 实现概率调整机制
✓ 公平性约束
✓ A/B测试准备

# 实现代码
from typing import List, Dict
import random

class SmartDealingStrategy:
    """智能发牌策略"""

    def __init__(self, config: dict):
        self.entertainment_weight = config.get("entertainment_weight", 0.3)
        self.fairness_constraint = config.get("fairness_constraint", 0.15)  # 最大调整15%
        self.player_analyzer = PlayerProfiler()

    async def deal_with_strategy(
        self,
        players: List[Player],
        game_context: dict
    ) -> Dict[str, List[Card]]:
        """策略性发牌"""
        # 1. 分析玩家状态
        player_states = await self._analyze_player_states(players)

        # 2. 计算发牌权重
        weights = self._calculate_dealing_weights(player_states, game_context)

        # 3. 应用公平性约束
        weights = self._apply_fairness_constraints(weights)

        # 4. 按权重发牌
        distribution = self._weighted_deal(players, weights)

        # 5. 记录发牌决策
        await self._log_dealing_decision(distribution, weights)

        return distribution

    async def _analyze_player_states(self, players: List[Player]) -> List[dict]:
        """分析玩家状态"""
        states = []

        for player in players:
            # 获取玩家统计
            stats = await redis.get(f"player:stats:{player.id}")
            stats = json.loads(stats) if stats else {}

            # 分析活跃度
            recent_games = await self._get_recent_games(player.id, n=10)
            activity_score = self._calculate_activity_score(recent_games)

            # 连续输赢情况
            streak = self._calculate_streak(recent_games)

            states.append({
                "player_id": player.id,
                "activity_score": activity_score,
                "streak": streak,
                "player_type": stats.get("player_type"),
                "skill_level": stats.get("skill_level", 50)
            })

        return states

    def _calculate_dealing_weights(
        self,
        player_states: List[dict],
        game_context: dict
    ) -> Dict[str, float]:
        """计算发牌权重"""
        weights = {}

        for state in player_states:
            player_id = state["player_id"]
            base_weight = 1.0

            # 活跃度调整
            if state["activity_score"] < 0.3:
                base_weight *= 1.1  # 不活跃玩家稍微提升

            # 连续输牌补偿
            if state["streak"]["type"] == "loss" and state["streak"]["count"] >= 5:
                compensation = min(state["streak"]["count"] * 0.02, 0.15)
                base_weight *= (1 + compensation)

            # 确保在公平性约束内
            base_weight = min(base_weight, 1 + self.fairness_constraint)

            weights[player_id] = base_weight

        return weights

    def _weighted_deal(
        self,
        players: List[Player],
        weights: Dict[str, float]
    ) -> Dict[str, List[Card]]:
        """加权发牌"""
        deck = Deck()
        deck.shuffle()

        # 评估每张牌的强度
        all_possible_hands = self._generate_all_possible_hands(deck.cards)
        hand_strengths = {
            tuple(hand): self._evaluate_hand_strength(hand)
            for hand in all_possible_hands
        }

        # 按权重分配牌
        distribution = {}
        used_cards = set()

        # 按权重从高到低排序玩家
        sorted_players = sorted(
            players,
            key=lambda p: weights.get(p.id, 1.0),
            reverse=True
        )

        for player in sorted_players:
            weight = weights.get(player.id, 1.0)

            # 根据权重选择牌力范围
            target_strength = self._map_weight_to_strength(weight)

            # 选择接近目标强度的牌
            best_hand = self._select_hand_near_strength(
                deck.cards,
                used_cards,
                target_strength,
                hand_strengths
            )

            distribution[player.id] = best_hand
            used_cards.update(best_hand)

        return distribution

    def _evaluate_hand_strength(self, hand: List[Card]) -> float:
        """评估手牌强度 (0-1)"""
        if len(hand) != 2:
            return 0.0

        # 简化的手牌强度评估
        ranks = [card.rank.value for card in hand]
        suited = hand[0].suit == hand[1].suit

        # 对子
        if ranks[0] == ranks[1]:
            return min(ranks[0] / 14, 1.0) * 0.9 + 0.1

        # 高牌
        high_card = max(ranks)
        low_card = min(ranks)

        strength = (high_card / 14) * 0.6 + (low_card / 14) * 0.3

        # 同花加成
        if suited:
            strength += 0.1

        # 连牌加成
        if abs(high_card - low_card) <= 3:
            strength += 0.05

        return min(strength, 1.0)

    def _apply_fairness_constraints(self, weights: Dict[str, float]) -> Dict[str, float]:
        """应用公平性约束"""
        # 确保整体平均权重接近1.0
        avg_weight = sum(weights.values()) / len(weights)

        if avg_weight > 1.05:  # 如果平均权重过高,进行归一化
            normalized_weights = {
                player_id: w / avg_weight
                for player_id, w in weights.items()
            }
            return normalized_weights

        return weights

    async def _log_dealing_decision(
        self,
        distribution: Dict[str, List[Card]],
        weights: Dict[str, float]
    ):
        """记录发牌决策 (用于后续分析和优化)"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "distribution": {
                player_id: [str(card) for card in cards]
                for player_id, cards in distribution.items()
            },
            "weights": weights,
            "algorithm_version": "v1.0"
        }

        await mongo_db.dealing_logs.insert_one(log_entry)
```

#### Week 18-20: 模拟测试环境
```python
# 任务清单
✓ 虚拟玩家系统
✓ 高速游戏模拟器
✓ 公平性验证工具
✓ 性能基准测试

# 实现代码
from multiprocessing import Pool
import numpy as np
from scipy import stats

class VirtualPlayer:
    """虚拟玩家 (AI)"""

    def __init__(self, player_type: PlayerType, skill_level: int):
        self.type = player_type
        self.skill_level = skill_level
        self.stack = 0
        self.hole_cards = []

    def make_decision(self, game_state: dict) -> dict:
        """根据玩家类型做决策"""
        if self.type == PlayerType.TAG:
            return self._tight_aggressive_strategy(game_state)
        elif self.type == PlayerType.LAG:
            return self._loose_aggressive_strategy(game_state)
        elif self.type == PlayerType.TIGHT_PASSIVE:
            return self._tight_passive_strategy(game_state)
        else:
            return self._loose_passive_strategy(game_state)

    def _tight_aggressive_strategy(self, game_state: dict) -> dict:
        """紧凶策略"""
        hand_strength = self._evaluate_current_hand(game_state)

        if hand_strength > 0.7:
            return {"action": "raise", "amount": game_state["pot"] * 0.75}
        elif hand_strength > 0.5:
            return {"action": "call"}
        else:
            return {"action": "fold"}

class GameSimulator:
    """游戏模拟器"""

    def __init__(self, dealing_engine: SmartDealingStrategy):
        self.engine = dealing_engine

    async def simulate_games(
        self,
        num_games: int,
        num_players: int = 6
    ) -> SimulationResult:
        """并行模拟大量游戏"""
        # 创建虚拟玩家
        players = self._create_virtual_players(num_players)

        # 使用进程池并行模拟
        with Pool(processes=8) as pool:
            results = pool.map(
                self._simulate_single_game_wrapper,
                [(players, i) for i in range(num_games)]
            )

        return SimulationResult(results)

    def _simulate_single_game(self, players: List[VirtualPlayer]) -> GameResult:
        """模拟单局游戏"""
        # 发牌
        distribution = self.engine.deal_with_strategy(players, {})

        # 分配底牌
        for player_id, cards in distribution.items():
            player = next(p for p in players if p.id == player_id)
            player.hole_cards = cards

        # 模拟下注回合
        pot = 0
        active_players = players.copy()

        for street in ["preflop", "flop", "turn", "river"]:
            # 公共牌
            if street != "preflop":
                community_cards = self._deal_community_cards(street)
            else:
                community_cards = []

            # 玩家决策
            for player in active_players:
                game_state = {
                    "pot": pot,
                    "community_cards": community_cards,
                    "street": street
                }

                decision = player.make_decision(game_state)

                if decision["action"] == "fold":
                    active_players.remove(player)
                elif decision["action"] == "call":
                    pot += decision.get("amount", 0)
                elif decision["action"] == "raise":
                    pot += decision["amount"]

        # 摊牌
        winner = self._determine_winner(active_players, community_cards)

        return GameResult(
            winner=winner,
            pot_size=pot,
            num_players=len(players)
        )

class FairnessValidator:
    """公平性验证器"""

    def validate_dealing_fairness(
        self,
        simulation_results: List[GameResult],
        confidence_level: float = 0.95
    ) -> ValidationReport:
        """验证发牌公平性"""
        report = ValidationReport()

        # 1. 卡方检验
        chi_square_result = self._chi_square_test(simulation_results)
        report.add_test("Chi-Square Test", chi_square_result)

        # 2. 手牌分布检验
        distribution_result = self._test_hand_distribution(simulation_results)
        report.add_test("Hand Distribution", distribution_result)

        # 3. 玩家胜率检验
        win_rate_result = self._test_win_rates(simulation_results)
        report.add_test("Win Rate Fairness", win_rate_result)

        return report

    def _chi_square_test(self, results: List[GameResult]) -> dict:
        """卡方检验"""
        # 统计每种牌型出现次数
        observed_freq = self._count_hand_types(results)

        # 理论期望频率
        total_hands = sum(observed_freq.values())
        expected_freq = {
            hand_type: total_hands * prob
            for hand_type, prob in HAND_TYPE_PROBABILITIES.items()
        }

        # 计算卡方统计量
        chi_square_stat = sum(
            (observed_freq[ht] - expected_freq[ht]) ** 2 / expected_freq[ht]
            for ht in observed_freq.keys()
        )

        # 自由度
        df = len(observed_freq) - 1

        # P值
        p_value = 1 - stats.chi2.cdf(chi_square_stat, df)

        return {
            "passed": p_value > 0.05,
            "chi_square": chi_square_stat,
            "p_value": p_value,
            "conclusion": "公平" if p_value > 0.05 else "存在偏差"
        }

# 概率常量
HAND_TYPE_PROBABILITIES = {
    "royal_flush": 0.000154,
    "straight_flush": 0.00139,
    "four_of_a_kind": 0.0240,
    "full_house": 0.1441,
    "flush": 0.1965,
    "straight": 0.3925,
    "three_of_a_kind": 2.1128,
    "two_pair": 4.7539,
    "one_pair": 42.2569,
    "high_card": 50.1177
}
```

### 2.3 第三阶段: AI训练与优化 (Week 21-32)

#### Week 21-24: 机器学习模型开发
```python
# 任务清单
✓ 数据采集与清洗
✓ 特征工程
✓ 模型训练
✓ 模型评估

# 完整的ML Pipeline
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# 1. 数据准备
class PlayerDataset(Dataset):
    """玩家数据集"""

    def __init__(self, features: np.ndarray, labels: np.ndarray):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# 2. 特征工程
class FeatureEngineering:
    """特征工程"""

    @staticmethod
    def extract_features(player_actions: List[ActionEvent]) -> np.ndarray:
        """提取特征向量"""
        features = []

        # 基础统计特征
        features.extend([
            len(player_actions),  # 总手数
            np.mean([a.amount for a in player_actions]),  # 平均下注额
            np.std([a.amount for a in player_actions]),  # 下注额标准差
        ])

        # VPIP特征
        preflop_actions = [a for a in player_actions if a.street == "preflop"]
        vpip = sum(1 for a in preflop_actions if a.action_type in ["call", "raise"]) / len(preflop_actions) if preflop_actions else 0
        features.append(vpip)

        # PFR特征
        pfr = sum(1 for a in preflop_actions if a.action_type == "raise") / len(preflop_actions) if preflop_actions else 0
        features.append(pfr)

        # 位置特征
        position_stats = {
            "early": [a for a in player_actions if a.position <= 2],
            "middle": [a for a in player_actions if 3 <= a.position <= 5],
            "late": [a for a in player_actions if a.position >= 6]
        }

        for pos, actions in position_stats.items():
            if actions:
                features.append(len(actions) / len(player_actions))  # 位置占比
            else:
                features.append(0)

        # 激进度特征
        aggressive_actions = sum(1 for a in player_actions if a.action_type in ["raise", "bet"])
        features.append(aggressive_actions / len(player_actions) if player_actions else 0)

        return np.array(features)

# 3. 深度学习模型
class PlayerBehaviorLSTM(nn.Module):
    """LSTM玩家行为预测模型"""

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        )
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # x shape: (batch, seq_len, input_dim)
        lstm_out, (hidden, cell) = self.lstm(x)
        # 取最后一个时间步的输出
        last_output = lstm_out[:, -1, :]
        x = self.fc1(last_output)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return self.softmax(x)

# 4. 训练管道
class ModelTrainer:
    """模型训练器"""

    def __init__(self, model: nn.Module, device: str = "cuda"):
        self.model = model.to(device)
        self.device = device
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            patience=5,
            factor=0.5
        )

    def train_epoch(self, train_loader: DataLoader) -> float:
        """训练一个epoch"""
        self.model.train()
        total_loss = 0

        for features, labels in train_loader:
            features = features.to(self.device)
            labels = labels.to(self.device)

            # 前向传播
            outputs = self.model(features)
            loss = self.criterion(outputs, labels)

            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    def evaluate(self, val_loader: DataLoader) -> dict:
        """评估模型"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                total_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        return {
            "loss": total_loss / len(val_loader),
            "accuracy": correct / total
        }

    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int = 50
    ):
        """完整训练流程"""
        best_val_loss = float('inf')

        for epoch in range(num_epochs):
            train_loss = self.train_epoch(train_loader)
            val_metrics = self.evaluate(val_loader)

            print(f"Epoch {epoch+1}/{num_epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss: {val_metrics['loss']:.4f}")
            print(f"  Val Accuracy: {val_metrics['accuracy']:.4f}")

            # 学习率调整
            self.scheduler.step(val_metrics['loss'])

            # 保存最佳模型
            if val_metrics['loss'] < best_val_loss:
                best_val_loss = val_metrics['loss']
                self.save_model("best_model.pth")

    def save_model(self, path: str):
        """保存模型"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

# 使用示例
async def train_player_behavior_model():
    """训练玩家行为预测模型"""
    # 1. 加载数据
    actions = await load_player_actions_from_db()

    # 2. 特征提取
    fe = FeatureEngineering()
    features = [fe.extract_features(player_actions) for player_actions in actions]
    labels = [get_player_label(player_actions) for player_actions in actions]

    # 3. 数据分割
    X_train, X_val, y_train, y_val = train_test_split(
        features, labels, test_size=0.2, random_state=42
    )

    # 4. 创建数据加载器
    train_dataset = PlayerDataset(X_train, y_train)
    val_dataset = PlayerDataset(X_val, y_val)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    # 5. 初始化模型
    model = PlayerBehaviorLSTM(input_dim=10, hidden_dim=128, output_dim=4)

    # 6. 训练
    trainer = ModelTrainer(model)
    trainer.train(train_loader, val_loader, num_epochs=50)
```

继续开发路线图...

#### Week 25-28: 强化学习优化发牌策略
```python
# 任务清单
✓ 构建RL环境
✓ 设计奖励函数
✓ 训练RL代理
✓ 策略评估

# Gym环境实现
import gym
from gym import spaces
import numpy as np

class PokerDealingEnv(gym.Env):
    """德州扑克发牌环境"""

    def __init__(self, num_players: int = 6):
        super().__init__()

        self.num_players = num_players

        # 动作空间: 每个玩家的权重调整 (-0.2 到 +0.2)
        self.action_space = spaces.Box(
            low=-0.2,
            high=0.2,
            shape=(num_players,),
            dtype=np.float32
        )

        # 观察空间: 玩家状态特征
        obs_dim = num_players * 10  # 每个玩家10个特征
        self.observation_space = spaces.Box(
            low=0,
            high=1,
            shape=(obs_dim,),
            dtype=np.float32
        )

        self.reset()

    def reset(self):
        """重置环境"""
        # 初始化玩家
        self.players = [VirtualPlayer(f"player_{i}") for i in range(self.num_players)]

        # 返回初始观察
        return self._get_observation()

    def step(self, action: np.ndarray):
        """执行一步"""
        # action是每个玩家的权重调整

        # 1. 应用权重发牌
        weights = {p.id: 1.0 + action[i] for i, p in enumerate(self.players)}
        distribution = self.dealing_engine.weighted_deal(self.players, weights)

        # 2. 模拟游戏
        game_result = self.simulate_game(distribution)

        # 3. 计算奖励
        reward = self._calculate_reward(game_result)

        # 4. 获取新状态
        obs = self._get_observation()

        # 5. 判断是否结束
        done = self.episode_steps >= self.max_episode_steps

        return obs, reward, done, {}

    def _calculate_reward(self, game_result: GameResult) -> float:
        """计算奖励"""
        reward = 0

        # 娱乐性奖励
        avg_actions = game_result.total_actions / self.num_players
        reward += avg_actions * 0.3  # 鼓励更多互动

        # 公平性惩罚
        if game_result.fairness_score < 0.9:
            reward -= (0.9 - game_result.fairness_score) * 10

        # 戏剧性奖励
        reward += game_result.dramatic_moments * 0.2

        return reward

    def _get_observation(self) -> np.ndarray:
        """获取当前观察"""
        obs = []
        for player in self.players:
            # 提取玩家特征
            features = [
                player.stats.vpip / 100,
                player.stats.pfr / 100,
                player.current_streak / 10,
                player.activity_score,
                # ... 更多特征
            ]
            obs.extend(features)

        return np.array(obs, dtype=np.float32)

# PPO算法实现
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

def train_dealing_policy():
    """训练发牌策略"""
    # 创建环境
    env = DummyVecEnv([lambda: PokerDealingEnv(num_players=6)])

    # 创建PPO代理
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log="./poker_dealing_tensorboard/"
    )

    # 训练
    model.learn(total_timesteps=1_000_000)

    # 保存模型
    model.save("ppo_dealing_policy")

    return model
```

#### Week 29-32: A/B测试与持续优化
```python
# 任务清单
✓ A/B测试框架
✓ 效果评估体系
✓ 自动化优化流程
✓ 模型版本管理

# A/B测试完整实现
import hashlib
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict

@dataclass
class ABTestConfig:
    """A/B测试配置"""
    test_name: str
    variants: List[str]
    traffic_split: List[float]
    start_time: datetime
    end_time: datetime
    primary_metric: str
    secondary_metrics: List[str]

class ABTestManager:
    """A/B测试管理器"""

    def __init__(self):
        self.active_tests: Dict[str, ABTestConfig] = {}
        self.results_collector = ResultsCollector()

    async def create_test(self, config: ABTestConfig) -> str:
        """创建A/B测试"""
        test_id = self._generate_test_id(config.test_name)

        # 保存到数据库
        await db.ab_tests.insert_one({
            "test_id": test_id,
            "test_name": config.test_name,
            "variants": config.variants,
            "traffic_split": config.traffic_split,
            "start_time": config.start_time,
            "end_time": config.end_time,
            "status": "active"
        })

        self.active_tests[test_id] = config
        return test_id

    def assign_variant(self, test_id: str, user_id: str) -> str:
        """为用户分配实验变体"""
        config = self.active_tests.get(test_id)
        if not config:
            return "control"

        # 一致性哈希
        hash_value = hashlib.md5(
            f"{user_id}:{test_id}".encode()
        ).hexdigest()
        hash_int = int(hash_value, 16)

        # 根据流量分配
        cumulative = 0
        rand_value = (hash_int % 10000) / 100  # 0-100

        for variant, split in zip(config.variants, config.traffic_split):
            cumulative += split
            if rand_value < cumulative:
                return variant

        return config.variants[0]

    async def record_metric(
        self,
        test_id: str,
        variant: str,
        metric_name: str,
        metric_value: float
    ):
        """记录指标数据"""
        await db.ab_test_results.insert_one({
            "test_id": test_id,
            "variant": variant,
            "metric_name": metric_name,
            "metric_value": metric_value,
            "recorded_at": datetime.now()
        })

    async def analyze_results(self, test_id: str) -> ABTestResult:
        """分析A/B测试结果"""
        config = self.active_tests.get(test_id)

        # 获取所有数据
        results = await db.ab_test_results.find({"test_id": test_id}).to_list(None)

        # 按变体分组
        variant_data = {}
        for variant in config.variants:
            variant_results = [r for r in results if r["variant"] == variant]
            variant_data[variant] = self._aggregate_metrics(variant_results)

        # 统计显著性检验
        significance = self._test_statistical_significance(variant_data)

        # 生成报告
        return ABTestResult(
            test_id=test_id,
            test_name=config.test_name,
            variant_data=variant_data,
            significance=significance,
            recommendation=self._generate_recommendation(significance)
        )

    def _test_statistical_significance(
        self,
        variant_data: Dict[str, dict]
    ) -> Dict[str, dict]:
        """统计显著性检验"""
        from scipy import stats

        results = {}
        control = variant_data.get("control")

        for variant_name, variant in variant_data.items():
            if variant_name == "control":
                continue

            # T检验
            t_stat, p_value = stats.ttest_ind(
                control["samples"],
                variant["samples"]
            )

            results[variant_name] = {
                "t_statistic": t_stat,
                "p_value": p_value,
                "is_significant": p_value < 0.05,
                "improvement": (variant["mean"] - control["mean"]) / control["mean"] * 100
            }

        return results

    def _generate_recommendation(self, significance: Dict) -> str:
        """生成推荐"""
        # 找出显著且改进最大的变体
        best_variant = None
        best_improvement = 0

        for variant, result in significance.items():
            if result["is_significant"] and result["improvement"] > best_improvement:
                best_variant = variant
                best_improvement = result["improvement"]

        if best_variant:
            return f"推荐使用 {best_variant}, 提升 {best_improvement:.2f}%"
        else:
            return "没有显著差异,保持当前版本"

# 自动化优化流程
class AutoOptimizer:
    """自动优化器"""

    def __init__(self):
        self.ab_test_manager = ABTestManager()
        self.model_trainer = ModelTrainer()

    async def run_optimization_cycle(self):
        """运行一个优化周期"""
        # 1. 收集数据
        recent_data = await self._collect_recent_data(days=7)

        # 2. 分析性能
        performance = self._analyze_performance(recent_data)

        # 3. 识别优化机会
        opportunities = self._identify_opportunities(performance)

        # 4. 生成新策略
        new_strategies = await self._generate_new_strategies(opportunities)

        # 5. 启动A/B测试
        for strategy in new_strategies:
            test_config = ABTestConfig(
                test_name=f"strategy_{strategy.version}",
                variants=["control", strategy.name],
                traffic_split=[0.5, 0.5],
                start_time=datetime.now(),
                end_time=datetime.now() + timedelta(days=7),
                primary_metric="player_engagement",
                secondary_metrics=["win_rate_variance", "session_length"]
            )

            test_id = await self.ab_test_manager.create_test(test_config)
            print(f"Started A/B test: {test_id}")

        # 6. 等待测试完成并分析
        await asyncio.sleep(7 * 24 * 3600)  # 7天后

        # 7. 分析结果并部署最优策略
        for test_id in test_ids:
            result = await self.ab_test_manager.analyze_results(test_id)
            if "推荐使用" in result.recommendation:
                await self._deploy_winning_variant(test_id, result)
```

### 2.4 第四阶段: 完善与优化 (Week 33-40)

#### Week 33-36: 性能优化
- 数据库查询优化
- 缓存策略优化
- 并发性能提升
- 资源使用优化

#### Week 37-38: 安全加固
- 安全审计
- 渗透测试
- 数据加密
- 访问控制完善

#### Week 39-40: 文档与培训
- API文档完善
- 开发者文档
- 运维手册
- 用户培训材料

### 2.5 第五阶段: 测试上线 (Week 41-44)

#### Week 41-42: 全面测试
- 单元测试覆盖率>80%
- 集成测试
- 端到端测试
- 性能压测

#### Week 43: 灰度发布
- 小流量测试
- 监控告警
- 问题修复

#### Week 44: 正式上线
- 全量发布
- 持续监控
- 问题响应

---

## 3. 总结

本文档详细规划了:
1. **完整技术栈**: 从后端到前端、从数据库到AI框架
2. **详细开发路线**: 44周的分阶段实施计划
3. **代码示例**: 关键模块的实现参考
4. **最佳实践**: DevOps、监控、测试等

建议团队规模: 5-8人 (2后端+1AI+1前端+1测试+1DevOps+PM)

预计总开发时间: 10-11个月
